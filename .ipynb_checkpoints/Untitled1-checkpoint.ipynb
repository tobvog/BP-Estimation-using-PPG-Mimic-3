{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0236d4",
   "metadata": {},
   "source": [
    "# Demonstration of ResNet with 2 Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c6ce8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12062a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from DataGenerator_Outp6 import DataGenerator\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow import  squeeze\n",
    "from tensorflow.keras.layers import Input, Conv1D, Reshape, LayerNormalization, ReLU, BatchNormalization, Add, AveragePooling1D, Flatten, Dense, GRU, concatenate, Dropout \n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6caed8",
   "metadata": {},
   "source": [
    "### Loading Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c427a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_main = \"C:/Users/vogel/Desktop/Study/Master BMIT/1.Semester/Programmierprojekt/feat_new/\"\n",
    "files = os.listdir(path_main+\"derivations/dev0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc2975",
   "metadata": {},
   "source": [
    "### Functions for building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bcc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Block\n",
    "def resnet_block(inp, batch_size, num_filter, kernel_sizes=[8,5,3], pool_size=3, pool_stride_size=2):\n",
    "    def conv_block(inp, batch_size, num_filter, kernel_size):\n",
    "        outp = Conv1D(kernel_size=kernel_size, filters=num_filter, padding=\"same\")(inp)\n",
    "        outp = BatchNormalization()(outp)\n",
    "        outp = ReLU()(outp)       \n",
    "        return outp\n",
    "    \n",
    "    def create_output_long(inp, batch_size, num_filter, kernel_sizes):\n",
    "        outp = inp\n",
    "        for i in range(0, len(kernel_sizes)):\n",
    "            outp = conv_block(outp, batch_size, num_filter, kernel_sizes[i])\n",
    "        return outp\n",
    "    \n",
    "    def create_output_short(inp, num_filter):\n",
    "        outp = Conv1D(kernel_size=1, filters=num_filter, padding=\"same\")(inp)\n",
    "        outp = BatchNormalization()(outp)\n",
    "        return outp\n",
    "    \n",
    "    outp_long = create_output_long(inp, batch_size, num_filter, kernel_sizes)\n",
    "    outp_short = create_output_short(inp, num_filter)\n",
    "\n",
    "    outp = Add()([outp_long, outp_short])\n",
    "\n",
    "    if len(outp.shape) == 4:     \n",
    "        outp = squeeze(outp, axis=1)\n",
    "\n",
    "    outp = AveragePooling1D(pool_size=pool_size, strides=pool_stride_size)(outp)\n",
    "    \n",
    "    return outp\n",
    "\n",
    "# Spectral Block\n",
    "def spec_temp_block(inp, batch_size):\n",
    "\n",
    "    outp = tf.signal.stft(inp, frame_length=128, frame_step=64)\n",
    "    outp = tf.abs(outp)\n",
    "    outp = LayerNormalization()(outp)\n",
    "    outp = Reshape((8*batch_size,65))(outp)\n",
    "    outp = GRU(65)(outp)\n",
    "    outp = Flatten()(outp)\n",
    "    outp = Dense(32, activation=\"relu\", kernel_regularizer=l2(0.001))(outp)\n",
    "    outp = BatchNormalization()(outp)\n",
    "    \n",
    "    return outp\n",
    "   \n",
    "# Create blocks of the network \n",
    "def net_blocks(batch_size):\n",
    "    inp_time0 = Input(shape=(batch_size, 624), name=\"input_time0\")\n",
    "    inp_spec0 = Input(shape=(batch_size, 624), name=\"input_spec0\")\n",
    "    inp_time1 = Input(shape=(batch_size, 624), name=\"input_time1\")\n",
    "    inp_spec1 = Input(shape=(batch_size, 624), name=\"input_spec1\")\n",
    "    inp_time2 = Input(shape=(batch_size, 624), name=\"input_time2\")\n",
    "    inp_spec2 = Input(shape=(batch_size, 624), name=\"input_spec2\")\n",
    "    \n",
    "    outp_time0 = resnet_block(inp_time0, batch_size=batch_size, num_filter=64)\n",
    "    outp_spec0 = spec_temp_block(inp_spec0, batch_size=batch_size)\n",
    "    outp_time1 = resnet_block(inp_time1, batch_size=batch_size, num_filter=64)\n",
    "    outp_spec1 = spec_temp_block(inp_spec1, batch_size=batch_size)\n",
    "    outp_time2 = resnet_block(inp_time2, batch_size=batch_size, num_filter=64)\n",
    "    outp_spec2 = spec_temp_block(inp_spec2, batch_size=batch_size)\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        outp_time0 = resnet_block(outp_time0, batch_size=batch_size, num_filter=128)\n",
    "        outp_time1 = resnet_block(outp_time1, batch_size=batch_size, num_filter=128)\n",
    "        outp_time2 = resnet_block(outp_time2, batch_size=batch_size, num_filter=128)\n",
    "        \n",
    "    model_time0 = keras.Model(inp_time0, outp_time0)#, name=model_names[0])\n",
    "    model_spec0 = keras.Model(inp_spec0, outp_spec0)#, name=model_names[0])\n",
    "    model_time1 = keras.Model(inp_time1, outp_time1)#, name=model_names[1])\n",
    "    model_spec1 = keras.Model(inp_spec1, outp_spec1)#, name=model_names[1])\n",
    "    model_time2 = keras.Model(inp_time2, outp_time2)#, name=model_names[2])\n",
    "    model_spec2 = keras.Model(inp_spec2, outp_spec2)#, name=model_names[2])\n",
    "        \n",
    "    return [model_time0, model_time1, model_time2], [model_spec0, model_spec1, model_spec2], [inp_time0, inp_time1, inp_time2], [inp_spec0, inp_spec1, inp_spec2]\n",
    "\n",
    "# Concatenate blocks of equal kind\n",
    "def concat_blocks(inp0, inp1, inp2, resblock): \n",
    "    outp = concatenate([inp0, inp1, inp2], axis=-1)\n",
    "    \n",
    "    if resblock==True:\n",
    "        outp = BatchNormalization()(outp)\n",
    "        outp = GRU(64)(outp)\n",
    "    outp = BatchNormalization()(outp)\n",
    "    \n",
    "    return  outp\n",
    "\n",
    "# Create Batches\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a34ee",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4647d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datagenerator\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataGenerator' object has no attribute 'path_main'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Generators\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Datagenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m generator_train \u001b[38;5;241m=\u001b[39m \u001b[43mDataGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m generator_val \u001b[38;5;241m=\u001b[39m DataGenerator(path_main, val_id, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     25\u001b[0m generator_test \u001b[38;5;241m=\u001b[39m DataGenerator(path_main, test_id, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32m~\\Desktop\\Study\\Master BMIT\\1.Semester\\Programmierprojekt\\Skript\\DataGenerator_Outp6.py:29\u001b[0m, in \u001b[0;36mDataGenerator.__init__\u001b[1;34m(self, path_main, list_id, batch_size, n_sample, n_classes, shuffle)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m## @brief Input data of actual subject.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dev0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_main\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivations/dev0/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_id[\u001b[38;5;241m0\u001b[39m], allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m## @brief First derivation of input data of actual subject.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dev1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_main\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivations/dev1/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_id[\u001b[38;5;241m0\u001b[39m], allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataGenerator' object has no attribute 'path_main'"
     ]
    }
   ],
   "source": [
    "# Initialize kfold\n",
    "n_splits = 2\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "kfold.get_n_splits(files)\n",
    "\n",
    "# Initialize Hyperparameter\n",
    "kernel_init = \"he_uniform\"\n",
    "batch_size = 256\n",
    "l2_lambda = 0.001   \n",
    "\n",
    "\n",
    "all_mae_sbp, all_mae_dbp = [], []\n",
    "for train_index, test_index in kfold.split(files): \n",
    "    \n",
    "    # Separate training, validation and test ids\n",
    "    train_index, val_index = train_test_split(train_index, test_size=0.2)\n",
    "    train_id = [files[x] for x in train_index]\n",
    "    val_id = [files[x] for x in val_index]\n",
    "    test_id = [files[x] for x in test_index]\n",
    "    \n",
    "    # Generators\n",
    "    print(\"Loading Datagenerator\")\n",
    "    generator_train = DataGenerator(path_main, train_id, batch_size=batch_size)\n",
    "    generator_val = DataGenerator(path_main, val_id, batch_size=batch_size)\n",
    "    generator_test = DataGenerator(path_main, test_id, batch_size=batch_size)\n",
    "\n",
    "    # Build ResNet blocks\n",
    "    print(\"Creating Model\")\n",
    "    model_time, model_spec, input_time, input_spec = net_blocks(batch_size)\n",
    "\n",
    "    \n",
    "    # Concatenate all ResNet and all Spetrogram blocks\n",
    "    concat_time = concat_blocks(input_time[0], input_time[1], input_time[2], resblock=True)\n",
    "    concat_spec = concat_blocks(input_spec[0], input_spec[1], input_spec[2], resblock=False)\n",
    "    \n",
    "    concat_time = Reshape((1, concat_time.shape[1]*concat_time.shape[2]))(concat_time)\n",
    "    concat_time = squeeze(concat_time, axis=1)\n",
    "    #outp_time1 = Reshape((1, outp_time.shape[1]*outp_time.shape[2]))(outp_time)\n",
    "    #outp_time1 = squeeze(outp_time, axis=1)\n",
    "    #outp_time2 = Reshape((1, outp_time.shape[1]*outp_time.shape[2]))(outp_time)\n",
    "    #outp_time2 = squeeze(outp_time, axis=1)\n",
    "    \n",
    "    # Concatenate ResNet and Spectrogram and build rest of the model\n",
    "    merged = concatenate([concat_time, concat_spec])\n",
    "    merged_outp = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_lambda), kernel_initializer=kernel_init)(merged)\n",
    "    merged_outp = Dropout(0.2)(merged_outp)\n",
    "    merged_outp = Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_lambda), kernel_initializer=kernel_init)(merged_outp)\n",
    "    merged_outp = Dropout(0.2)(merged_outp)\n",
    "    merged_outp = Dense(2, activation='relu')(merged_outp)\n",
    "\n",
    "    final_model = keras.Model(inputs=[input_time0,\n",
    "                                    input_time1,\n",
    "                                    input_time2,\n",
    "                                    input_spec0,   \n",
    "                                    input_spec1,      \n",
    "                                    input_spec2],\n",
    "                                    outputs=merged_outp,\n",
    "                                    name='Final_Model')\n",
    "    \n",
    "    \n",
    "    optimizer = optimizers.RMSprop(learning_rate=0.0001)\n",
    "    \n",
    "    es = EarlyStopping(monitor=\"mae\", patience=5)\n",
    "    final_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    # print(final_model.summary())\n",
    "    \n",
    "    final_model.fit(generator_train, \n",
    "                    validation_data=generator_val,\n",
    "                    epochs=1,\n",
    "                    verbose=1, \n",
    "                    callbacks=[es])\n",
    "    \n",
    "\n",
    "    all_pred = []\n",
    "    all_true = []\n",
    "    for batch_index in range(generator_test.__len__()):\n",
    "        print(batch_index)\n",
    "        batch_data, batch_true_labels = generator_test.__getitem__(batch_index)\n",
    "\n",
    "        batch_pred = final_model.predict(batch_data, verbose=0)\n",
    "        all_pred.append(batch_pred)\n",
    "        sbp_mean = np.mean(batch_true_labels[:,0])\n",
    "        dbp_mean = np.mean(batch_true_labels[:,1])\n",
    "        all_true.append(np.array([sbp_mean, dbp_mean]))\n",
    "\n",
    "    all_pred = np.concatenate(all_pred, axis=0)\n",
    "    all_true = np.array(all_true)\n",
    "\n",
    "    mae_sbp = mean_absolute_error(all_pred[:,0], all_true[:,0])\n",
    "    mae_dbp = mean_absolute_error(all_pred[:,1], all_true[:,1])\n",
    "\n",
    "    all_mae_sbp.append(mae_sbp)\n",
    "    all_mae_dbp.append(mae_dbp)\n",
    "\n",
    "mae_sbp_mean = np.mean(all_mae_sbp)\n",
    "mae_dbp_mean = np.mean(all_mae_dbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba7814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
